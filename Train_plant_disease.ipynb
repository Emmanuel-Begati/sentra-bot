{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5169432f-127b-417f-a4a4-8d3d822b9cf1",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2e3912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, Input, Layer, MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.applications import ResNet50, ResNet101\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be04aab1",
   "metadata": {},
   "source": [
    "#### Dataset Link: https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35eca78",
   "metadata": {},
   "source": [
    "## Enhanced Data Preprocessing with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba22440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size - using 224x224 for compatibility with ResNet\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Data augmentation for training images\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip('horizontal'),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "    tf.keras.layers.RandomContrast(0.1),\n",
    "])\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "# Function to prepare datasets with caching and prefetching for performance\n",
    "def prepare_dataset(ds, augment=False, cache=True):\n",
    "    ds = ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "        \n",
    "    ds = ds.shuffle(1000)\n",
    "    \n",
    "    if augment:\n",
    "        ds = ds.map(\n",
    "            lambda x, y: (data_augmentation(x, training=True), y),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "    \n",
    "    return ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baf7982",
   "metadata": {},
   "source": [
    "### Training Dataset with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec18c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    'train',\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")\n",
    "\n",
    "# Enhance training dataset with augmentation\n",
    "train_ds = prepare_dataset(training_set, augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64a3acf",
   "metadata": {},
   "source": [
    "### Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f508975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    'valid',\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")\n",
    "\n",
    "# Prepare validation dataset (no augmentation)\n",
    "val_ds = prepare_dataset(validation_set, augment=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e24161",
   "metadata": {},
   "source": [
    "### Visualize Augmented Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0920971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some augmented training images\n",
    "def visualize_augmented_images():\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for images, labels in train_ds.take(1):\n",
    "        for i in range(9):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(images[i])\n",
    "            class_names = training_set.class_names\n",
    "            class_index = tf.argmax(labels[i])\n",
    "            plt.title(class_names[class_index])\n",
    "            plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_augmented_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a6315f",
   "metadata": {},
   "source": [
    "## 1. Building ResNet-Based Model with Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e68e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet_model(num_classes):\n",
    "    # Use ResNet50 as base model, pre-trained on ImageNet\n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Build model architecture\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b6b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of classes\n",
    "num_classes = len(training_set.class_names)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "# Create the model\n",
    "resnet_model = build_resnet_model(num_classes)\n",
    "\n",
    "# Compile the model\n",
    "resnet_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c18787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0478153f",
   "metadata": {},
   "source": [
    "### Callbacks for Better Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680cfe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for better training\n",
    "callbacks = [\n",
    "    # Save best model during training\n",
    "    ModelCheckpoint(\n",
    "        'best_resnet_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Early stopping to prevent overfitting\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Reduce learning rate when plateau is reached\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24abdcd8",
   "metadata": {},
   "source": [
    "### Training ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "resnet_history = resnet_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a04f62",
   "metadata": {},
   "source": [
    "### Fine-tuning ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccaf7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the top layers for fine-tuning\n",
    "base_model = resnet_model.layers[0]\n",
    "for layer in base_model.layers[-30:]:  # Unfreeze last 30 layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "resnet_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Lower learning rate for fine-tuning\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Continue training with fine-tuning\n",
    "fine_tune_history = resnet_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c642ad3",
   "metadata": {},
   "source": [
    "## 2. Building Vision Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8260739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformer encoder block\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"gelu\"), \n",
    "             Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# Patch encoder layer\n",
    "class PatchEncoder(Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = Dense(units=projection_dim)\n",
    "        self.position_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b58ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vit_model(num_classes, image_size=224, patch_size=16, projection_dim=768, num_heads=12, transformer_layers=12):\n",
    "    # Input layer\n",
    "    inputs = Input(shape=(image_size, image_size, 3))\n",
    "    \n",
    "    # Patch extraction\n",
    "    patches = tf.keras.layers.Conv2D(filters=projection_dim, kernel_size=patch_size, strides=patch_size, padding=\"VALID\")(inputs)\n",
    "    patches_shape = patches.shape\n",
    "    num_patches = (image_size // patch_size) ** 2\n",
    "    patches = tf.reshape(patches, (-1, num_patches, projection_dim))\n",
    "    \n",
    "    # Patch encoding\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "    \n",
    "    # Create transformer blocks\n",
    "    for _ in range(transformer_layers):\n",
    "        encoded_patches = TransformerBlock(projection_dim, num_heads, projection_dim*4)(encoded_patches)\n",
    "    \n",
    "    # Create classifier head\n",
    "    representation = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = tf.keras.layers.GlobalAvgPool1D()(representation)\n",
    "    representation = Dropout(0.3)(representation)\n",
    "    features = Dense(projection_dim, activation=\"gelu\")(representation)\n",
    "    features = Dropout(0.3)(features)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(features)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23620b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simplified ViT model (to manage computational resources)\n",
    "vit_model = build_vit_model(\n",
    "    num_classes=num_classes, \n",
    "    image_size=IMG_SIZE, \n",
    "    patch_size=32,  # Larger patch size for efficiency\n",
    "    projection_dim=128,  # Smaller projection dimension\n",
    "    num_heads=4,  # Fewer attention heads\n",
    "    transformer_layers=4  # Fewer transformer layers\n",
    ")\n",
    "\n",
    "# Compile ViT model\n",
    "vit_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a0411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the ViT model\n",
    "vit_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158482bb",
   "metadata": {},
   "source": [
    "### Training Vision Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f29a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new callbacks for the ViT model\n",
    "vit_callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        'best_vit_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=6,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        verbose=1,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train the ViT model\n",
    "vit_history = vit_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=vit_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cac7941",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45994db2",
   "metadata": {},
   "source": [
    "### Compare Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a0027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(resnet_history, vit_history):\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # Plot training & validation accuracy values\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(resnet_history.history['accuracy'], label='ResNet Train')\n",
    "    plt.plot(resnet_history.history['val_accuracy'], label='ResNet Validation')\n",
    "    if vit_history:\n",
    "        plt.plot(vit_history.history['accuracy'], label='ViT Train')\n",
    "        plt.plot(vit_history.history['val_accuracy'], label='ViT Validation')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(resnet_history.history['loss'], label='ResNet Train')\n",
    "    plt.plot(resnet_history.history['val_loss'], label='ResNet Validation')\n",
    "    if vit_history:\n",
    "        plt.plot(vit_history.history['loss'], label='ViT Train')\n",
    "        plt.plot(vit_history.history['val_loss'], label='ViT Validation')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history comparison\n",
    "plot_training_history(resnet_history, vit_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b6b304",
   "metadata": {},
   "source": [
    "### Evaluate Models on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3daddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ResNet model\n",
    "print(\"ResNet Model Evaluation:\")\n",
    "resnet_val_loss, resnet_val_acc = resnet_model.evaluate(val_ds)\n",
    "print(f\"Validation Loss: {resnet_val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {resnet_val_acc:.4f}\")\n",
    "\n",
    "# Evaluate ViT model\n",
    "print(\"\\nVision Transformer Model Evaluation:\")\n",
    "vit_val_loss, vit_val_acc = vit_model.evaluate(val_ds)\n",
    "print(f\"Validation Loss: {vit_val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {vit_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461b2b5",
   "metadata": {},
   "source": [
    "### Confusion Matrix for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be29a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Choose the best model (assuming ResNet is better)\n",
    "best_model = resnet_model if resnet_val_acc > vit_val_acc else vit_model\n",
    "best_model_name = \"ResNet\" if resnet_val_acc > vit_val_acc else \"Vision Transformer\"\n",
    "\n",
    "# Create a non-shuffled validation dataset to get true order of images\n",
    "test_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    'valid',\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=False\n",
    ")\n",
    "test_ds = prepare_dataset(test_set, augment=False)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(test_ds)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Get true labels\n",
    "true_categories = tf.concat([y for x, y in test_ds], axis=0).numpy()\n",
    "true_classes = np.argmax(true_categories, axis=1)\n",
    "\n",
    "# Get class names\n",
    "class_names = test_set.class_names\n",
    "\n",
    "# Print classification report\n",
    "print(f\"Classification Report for {best_model_name} Model:\")\n",
    "print(classification_report(true_classes, y_pred_classes, target_names=class_names))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(true_classes, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(f\"Confusion Matrix - {best_model_name} Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76314241",
   "metadata": {},
   "source": [
    "### Save the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc6401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "best_model.save(\"best_plant_disease_model.keras\")\n",
    "print(f\"Best model ({best_model_name}) saved as best_plant_disease_model.keras\")\n",
    "\n",
    "# Save model architecture as JSON\n",
    "import json\n",
    "model_json = best_model.to_json()\n",
    "with open(\"best_model_architecture.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "print(\"Model architecture saved to best_model_architecture.json\")\n",
    "\n",
    "# Save class names\n",
    "with open(\"class_names.json\", \"w\") as f:\n",
    "    json.dump(class_names, f)\n",
    "print(\"Class names saved to class_names.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d770d1e",
   "metadata": {},
   "source": [
    "## Visualize Predictions on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706e68c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, dataset, num_images=8):\n",
    "    # Get a batch of images and labels\n",
    "    for images, labels in dataset.take(1):\n",
    "        # Make predictions\n",
    "        predictions = model.predict(images)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        true_classes = np.argmax(labels, axis=1)\n",
    "        \n",
    "        # Plot images with predictions\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for i in range(min(num_images, len(images))):\n",
    "            plt.subplot(2, 4, i+1)\n",
    "            plt.imshow(images[i])\n",
    "            \n",
    "            # Highlight correct and wrong predictions\n",
    "            color = \"green\" if predicted_classes[i] == true_classes[i] else \"red\"\n",
    "            confidence = predictions[i][predicted_classes[i]] * 100\n",
    "            \n",
    "            plt.title(f\"True: {class_names[true_classes[i]]}\\nPred: {class_names[predicted_classes[i]]}\\nConf: {confidence:.1f}%\", \n",
    "                      color=color)\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "# Visualize predictions from the best model\n",
    "print(f\"Predictions with {best_model_name} model:\")\n",
    "visualize_predictions(best_model, val_ds, num_images=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565d9b7a",
   "metadata": {},
   "source": [
    "## Model for Mobile Deployment (Optimized Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc156b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lightweight_model(num_classes):\n",
    "    \"\"\"Build a lightweight model for mobile deployment\"\"\"\n",
    "    model = Sequential([\n",
    "        # First block\n",
    "        Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        MaxPool2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Second block\n",
    "        Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPool2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Third block\n",
    "        Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPool2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Classifier\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Build and compile lightweight model\n",
    "mobile_model = build_lightweight_model(num_classes)\n",
    "mobile_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Summary\n",
    "mobile_model.summary()\n",
    "\n",
    "# Optional: Train the mobile model if you want to deploy it\n",
    "# mobile_history = mobile_model.fit(train_ds, validation_data=val_ds, epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540fb4a",
   "metadata": {},
   "source": [
    "## Convert Model to TFLite for Mobile Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08911d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tflite(model, quantize=True, model_name=\"plant_disease_model.tflite\"):\n",
    "    \"\"\"Convert model to TFLite format with optional quantization\"\"\"\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    \n",
    "    if quantize:\n",
    "        # Quantize model to reduce size\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        model_name = \"plant_disease_model_quantized.tflite\"\n",
    "    \n",
    "    # Convert the model\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    # Save the model\n",
    "    with open(model_name, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    # Print model size\n",
    "    print(f\"Model saved as {model_name}\")\n",
    "    print(f\"Model size: {os.path.getsize(model_name) / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "# Convert best model to TFLite format (uncomment to run)\n",
    "# convert_to_tflite(best_model, quantize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
